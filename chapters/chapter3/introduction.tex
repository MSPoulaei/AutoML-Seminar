\section{مقدمه}
ادغام مدل‌های زبانی بزرگ در \persianfootnote{خودکارسازی یادگیری ماشین}\LTRfootnote{Automated Machine Learning (AutoML)} به‌عنوان رویکردی دگرگون‌ساز برای بهینه‌سازی \persianfootnote{معماری‌های عصبی}\LTRfootnote{neural architectures} و \persianfootnote{فراپارامترها}\LTRfootnote{hyperparameters} پدیدار شده است. این فصل کارهای موجود را از سه منظر مکمل بررسی می‌کند: \persianfootnote{معماری عامل}\LTRfootnote{agent architecture}، \persianfootnote{راهبردهای تقویت دانش}\LTRfootnote{knowledge augmentation strategies}، و طراحی قالب خروجی. این ابعاد در کنار هم نشان می‌دهند که سامانه‌های گوناگون چگونه از قابلیت‌های مدل‌های زبانی بزرگ بهره می‌برند و هم‌زمان محدودیت‌های ذاتی آن‌ها را مدیریت می‌کنند.

\section{ظهور نخستین روش‌های یادگیری ماشین خودکار}
خودکارسازی یادگیری ماشین با هدف \persianfootnote{مردمی‌سازی}\LTRfootnote{democratization} فرایندهای پرهزینه و زمان‌بر \persianfootnote{گزینش مدل}\LTRfootnote{model selection} و \persianfootnote{بهینه‌سازی فراپارامتر}\LTRfootnote{hyperparameter optimization (HPO)} پدید آمد. ریشه گزینش الگوریتم به سال 1976 بازمی‌گردد که در آن، مسئله انتخاب الگوریتم بهینه از میان یک مجموعه ازپیش‌تعریف‌شده به‌منظور کمینه‌سازی افت کارایی روی داده‌های اعتبارسنجی صورت‌بندی شد \cite{RICE197665}. کوشش‌های نخستین در حوزه بهینه‌سازی فراپارامتر بر روش‌های منفردی چون \persianfootnote{جست‌وجوی شبکه‌ای}\LTRfootnote{grid search} و \persianfootnote{جست‌وجوی تصادفی}\LTRfootnote{random search} متمرکز بود؛ روش‌هایی ساده اما ناکارا در فضاهای بعدبالا \cite{JMLR:v13:bergstra12a}. این روش نشان داد که جست‌وجوی تصادفی با تخصیص مؤثرتر منابع به فراپارامترهای اثرگذار، از جست‌وجوی شبکه‌ای پیشی می‌گیرد و بدین‌ترتیب زمینه را برای روش‌های پیشرفته‌تر فراهم ساخت \cite{JMLR:v13:bergstra12a}.

نقطه عطف در 2013 با معرفی مسئله CASH رخ داد؛ چارچوبی که گزینش الگوریتم و بهینه‌سازی فراپارامتر را در یک بهینه‌سازی واحد ادغام می‌کرد \cite{10.1145/2487575.2487629}. بر مبنای این ایده، Auto-WEKA به‌عنوان نخستین سامانه جامع یادگیری ماشین خودکار پدید آمد و با اتکا به SMAC — رویکردی مبتنی بر \persianfootnote{بهینه‌سازی بیزی}\LTRfootnote{Bayesian optimization} که از \persianfootnote{جنگل‌های تصادفی}\LTRfootnote{random forests} به‌عنوان \persianfootnote{جانشین}\LTRfootnote{surrogate} استفاده می‌کرد — به جست‌وجو در میان رده‌بندهای WEKA و فراپارامترهایشان پرداخت \cite{10.1145/2487575.2487629, 10.1007/978-3-642-25566-3_40}. در ادامه، auto-sklearn در 2015 پارادایم CASH را به الگوریتم‌های scikit-learn بسط داد و با به‌کارگیری \persianfootnote{فرا-یادگیری}\LTRfootnote{meta-learning} برای \persianfootnote{آغاز گرم}\LTRfootnote{warm-starting} و نیز ساخت \persianfootnote{هم‌بندی}\LTRfootnote{ensemble}‌های مقاوم، بهبودهای معناداری رقم زد \cite{NIPS2015_11d0e628}.

به‌طور موازی، \persianfootnote{الگوریتم‌های تکاملی}\LTRfootnote{evolutionary algorithms} به‌عنوان بدیلی تواناتر مطرح شدند؛ چنان‌که TPOT \persianfootnote{خطوط لوله}\LTRfootnote{ML pipelines} یادگیری ماشین را به‌صورت برنامه‌هایی درخت‌ساختار مدل می‌کند و با \persianfootnote{برنامه‌نویسی ژنتیکی}\LTRfootnote{genetic programming}، ترکیب‌های انعطاف‌پذیری از پیش‌پردازنده‌ها، \persianfootnote{گزیننده‌های ویژگی}\LTRfootnote{feature selectors} و مدل‌ها را می‌آزماید \cite{pmlr-v64-olson_tpot_2016}. روش‌های \persianfootnote{چندسطوحی/چندوفایی}\LTRfootnote{multi-fidelity} با تخصیص تدریجی منابع به پیکربندی‌های امیدبخش و با الهام از \persianfootnote{راهبردهای باندیتی}\LTRfootnote{bandit-based strategies} هزینه محاسباتی را مهار کردند \cite{pmlr-v51-jamieson16, JMLR:v18:16-558}. تمرکز این رویکردهای اولیه عمدتاً بر مسائل کلاسیک \persianfootnote{طبقه‌بندی}\LTRfootnote{classification} و \persianfootnote{رگرسیون}\LTRfootnote{regression} بود؛ و معیارهایی چون \lr{OpenML-CC18} ارزیابی‌های استانداردشده را تسهیل کردند \cite{bischl2021openmlbenchmarkingsuites}.

گذار به \persianfootnote{جست‌وجوی معماری شبکه‌های عصبی}\LTRfootnote{Neural Architecture Search (NAS)} از حوالی 2017 آغاز شد و طراحی معماری را امتداد طبیعی بهینه‌سازی فراپارامتر تلقی کرد. کارهای اولیه مبتنی بر \persianfootnote{یادگیری تقویتی}\LTRfootnote{reinforcement learning (RL)} در جست‌وجوی معماری شبکه‌های عصبی، گرچه پرهزینه، نقطه شروعی اثرگذار بودند و گونه‌های کاراتری همچون ENAS با \persianfootnote{اشتراک پارامتر}\LTRfootnote{parameter sharing} را الهام بخشیدند \cite{zoph2017neural, pmlr-v80-pham18a}. در ادامه، معیارهایی مانند \lr{NAS-Bench-101} با فراهم‌کردن ارزیابی‌های ازپیش‌محاسبه‌شده، امکان پژوهش بازتولیدپذیر را مهیا کردند \cite{pmlr-v97-ying19a}. این سیر تحول، تنظیم دستی را به خطوط لوله خودکار بدل کرد و بستر را برای همگرایی با پارادایم‌های پیشرفته‌تر فراهم ساخت.