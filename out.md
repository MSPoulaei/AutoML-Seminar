### 3.1 Introduction

Automated Machine Learning (AutoML) has revolutionized the field of machine learning by automating the construction and optimization of ML pipelines, thereby reducing the dependency on specialized human expertise and enabling broader accessibility to advanced data-driven solutions. Traditional AutoML approaches primarily address challenges in data preparation, feature engineering, hyperparameter optimization (HPO), and neural architecture search (NAS), with frameworks such as Auto-WEKA and Auto-sklearn exemplifying early efforts to streamline model selection and configuration through meta-learning and ensemble techniques. These methods have achieved notable success in tasks like image classification and tabular data analysis, but they often require substantial computational resources and predefined search spaces, limiting their flexibility in dynamic or knowledge-intensive environments.

The emergence of Large Language Models (LLMs), such as the GPT series and BERT, marks a transformative shift in AutoML, leveraging their advanced natural language understanding, reasoning capabilities, and ability to integrate vast pre-trained knowledge to enhance automation across the ML lifecycle. LLMs facilitate human-machine interaction through conversational interfaces, enabling non-experts to specify tasks via natural language prompts while automating complex decisions like search space definition and performance prediction. For instance, LLMs can serve as surrogates in optimization processes, generate interpretable explanations of AutoML outcomes, and incorporate external knowledge to refine configurations, addressing limitations in traditional systems such as high resource consumption and lack of interpretability.

This review synthesizes the evolution of AutoML, beginning with foundational methods and progressing to LLM-integrated approaches that emphasize agent-based architectures, external knowledge utilization, and diverse output formats. We examine single- and multi-agent systems for collaborative optimization, strategies like retrieval-augmented generation and search-based augmentation for incorporating domain knowledge, and output modalities including vocabulary-based responses, code generation, tree structures, and hybrid forms. Finally, we classify related works to highlight trends and gaps, drawing on benchmarks like NAS-Bench and MLAgentBench to evaluate performance and generalizability. While LLM-enhanced AutoML presents opportunities for efficiency and democratization, it also introduces challenges such as prompt engineering complexities, hallucination risks, and ethical considerations, which warrant further investigation.

---

Automated Machine Learning (AutoML) represents a paradigm shift in the design and deployment of machine learning systems, aiming to automate the end-to-end process of applying ML to real-world problems without extensive manual intervention. As outlined in comprehensive surveys, traditional AutoML encompasses several core components: data preparation (including collection, cleaning, and augmentation), feature engineering, model generation via HPO and NAS, and evaluation. Early frameworks like Auto-WEKA focused on combined algorithm selection and hyperparameter optimization (CASH), employing techniques such as Bayesian optimization and meta-learning to navigate vast search spaces efficiently. Similarly, Auto-sklearn integrated ensemble methods and surrogate models to achieve robust performance across diverse datasets, as evidenced by its applications in tabular and image data tasks. These approaches have been instrumental in addressing the "no free lunch" theorem by tailoring pipelines to specific data distributions, yet they often suffer from high computational overhead and limited adaptability to unstructured or multimodal data.

The integration of Large Language Models (LLMs) into AutoML pipelines has ushered in a new era, capitalizing on their pre-trained knowledge from vast corpora to enhance reasoning, interaction, and generation capabilities. LLMs like GPT-4 and LLaMA enable symbiotic enhancements: they can act as interfaces for user-friendly interactions, configure search spaces dynamically, and replace traditional components such as performance predictors with meta-learned surrogates derived from unstructured sources (e.g., research papers and OpenML repositories). For example, systems like GPT-NAS and GENIUS utilize LLMs for evolutionary NAS, iteratively refining architectures through prompting and feedback loops, while CAAFE employs LLMs for context-aware feature engineering with explanatory code generation. In data engineering, LLMs facilitate adaptive cleaning and augmentation, such as in AutoMÂ³L, where they generate tailored modules for imputation and transformation based on dataset summaries. Model selection benefits from retrieval-based "model zoos" (e.g., in AutoMMLab) and generation-based synthesis (e.g., ModelGPT for customized architectures via LoRA fine-tuning). HPO is advanced through execution-based iterations (e.g., HPO-LLaMA) and prediction-based warm-starting (e.g., LLAMBO framing Bayesian optimization in natural language).

Opportunities abound in LLM-enhanced AutoML, including improved human-AI collaboration via chatbots (e.g., ChatGPT-inspired interfaces), interpretability through textual explanations, and resource efficiency in green AutoML paradigms. Hybrid systems combining LLMs with traditional optimizers promise to mitigate biases and enhance generalizability across domains like computer vision and graph learning. However, challenges persist, such as data leakage from pre-training, the need for sophisticated prompt engineering to handle token limits and heterogeneous data, hallucinations leading to invalid configurations, and high inference costs requiring lightweight alternatives. Ethical issues, including bias propagation and labor displacement, underscore the importance of robust validation, reinforcement learning from human feedback (RLHF), and knowledge graph integration to ensure factuality.

This chapter organizes the literature review as follows: Subsection 3.1.1 discusses the emergence of initial AutoML methods, drawing on foundational works in HPO and NAS. Subsection 3.1.2 explores AutoML advancements in the LLM era, highlighting agent-driven innovations. Subsequent sections provide analyses based on agent architectures (single vs. multi-agent), external knowledge integration (e.g., retrieval-augmented and search-based), output formats (vocabulary, code, tree, and combined), and a comprehensive classification of related works, including tabular comparisons for LLM-based methods.

| Aspect | Traditional AutoML | LLM-Enhanced AutoML |
|--------|-------------------|---------------------|
| **Human Interaction** | Limited to predefined APIs; requires ML expertise | Conversational interfaces; accessible to non-experts via natural language prompts |
| **Knowledge Integration** | Relies on meta-learning from structured data | Leverages pre-trained knowledge from unstructured corpora; supports external tools like knowledge graphs |
| **Optimization Efficiency** | Iterative search (e.g., Bayesian, evolutionary) often resource-intensive | Predictive surrogates and warm-starting reduce evaluations; hybrid with traditional methods for speed |
| **Interpretability** | Black-box outcomes; limited explanations | Textual reports and Q&A; generates human-readable code and features |
| **Challenges** | Scalability in search spaces; data-specific tuning | Hallucinations, prompt complexity, ethical biases |

This structured analysis aims to provide a holistic view of the field's progression, identifying avenues for future research in reliable, efficient, and ethical AutoML systems.

**Key Citations:**
- Tornede et al. (2023): AutoML in the Age of Large Language Models: Current Challenges, Future Opportunities and Risks. arXiv:2306.08107.
- Wang et al. (2024): Large Language Models for Constructing and Optimizing Machine Learning Pipelines: A Survey. arXiv:2411.10478.
- He et al. (2021): AutoML: A survey of the state-of-the-art. Knowledge-Based Systems, 212, 106622.
- Hutter et al. (2019): Automated Machine Learning: Methods, Systems, Challenges. Springer.
- Feurer et al. (2015): Efficient and Robust Automated Machine Learning. Advances in Neural Information Processing Systems.